{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDOA2v_-vIlI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample(data):\n",
    "  all_user = set(data[\"userId\"])\n",
    "  user_cnt = data[\"userId\"].value_counts().to_dict()\n",
    "\n",
    "  train, val = pd.DataFrame(columns=['userId', 'movieId', 'rating', 'timestamp', 'datetime'], dtype=object), pd.DataFrame(columns=['userId', 'movieId', 'rating', 'timestamp', 'datetime'], dtype=object)\n",
    "  for user in all_user:\n",
    "    count = user_cnt[user]\n",
    "    sub = data[data['userId'] == user]\n",
    "    sub_train, sub_val = train_test_split(sub, test_size=0.2, random_state=2022)\n",
    "    train = train.append(sub_train, ignore_index=True)\n",
    "    val = val.append(sub_val, ignore_index=True)\n",
    "  # print(train)\n",
    "  # print(val)\n",
    "  return train, val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "path = 'small'\n",
    "ratings = pd.read_csv(os.path.join(path, \"ratings.csv\"), sep=\",\", nrows=2000)\n",
    "ratings[\"datetime\"] = ratings[\"timestamp\"].map(lambda x: datetime.fromtimestamp(x))\n",
    "num_users = len(set(ratings['userId']))\n",
    "print(ratings.columns)\n",
    "train_data, val_data = sample(ratings)\n",
    "print(num_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tags = pd.read_csv(os.path.join(path, \"tags.csv\"), sep=\",\", nrows=2000)\n",
    "tags['datetime'] = tags['timestamp'].map(lambda x: datetime.fromtimestamp(x))\n",
    "tags.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movies = pd.read_csv(os.path.join(path, \"movies.csv\"), sep=\",\", nrows=None)\n",
    "movies.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = '\\(\\d\\d\\d\\d\\)'\n",
    "\n",
    "movie_title = movies['title']\n",
    "\n",
    "# title = movie_title.map(lambda x: re.sub(pattern, \" \", x))\n",
    "# year = movie_title.map(lambda x: re.findall(\"\\d\\d\\d\\d\", x)[-1] if re.search(\"\\d\\d\\d\\d\", x) is not None else \"0000\")\n",
    "# print(year)\n",
    "movie_year = {}\n",
    "movie_dict = {}\n",
    "movieIdset = set(movies[\"movieId\"])\n",
    "for movieId in movieIdset:\n",
    "  sub_tags = tags[tags['movieId']==movieId]\n",
    "  sub_movies = movies[movies['movieId']==movieId]\n",
    "  if movieId not in movie_dict:\n",
    "    movie_dict[movieId] = {}\n",
    "  title = sub_movies['title'].item()\n",
    "  movie_dict[movieId]['title'] = re.sub(pattern, \" \", title)\n",
    "  movie_dict[movieId]['genres'] = list(sub_movies['genres'])[0].replace(\"|\", \" \")\n",
    "  if len(sub_tags) > 0:\n",
    "    if \"tag\" not in movie_dict[movieId]:\n",
    "      movie_dict[movieId]['tag'] = \"\"\n",
    "    movie_dict[movieId]['tag'] += \" \".join(list(sub_tags['tag']))\n",
    "  movie_year[movieId] = int(re.findall(\"\\d\\d\\d\\d\", title)[-1]) if re.search(\"\\d\\d\\d\\d\", title) is not None else 1800\n",
    "# print(movie_dict)\n",
    "# print(movie_year)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movie_text = {}\n",
    "for movieId, info in movie_dict.items():\n",
    "  movie_text[movieId] = info['title'].strip() + \" \" + info['genres'].strip() + ((\" tags \"+\" \".join(set(info['tag'].strip().split(\" \")))) if 'tag' in info else \"\")\n",
    "# print(movie_text)\n",
    "# print(max([len(x.split(\" \")) for x in movie_text.values()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# word tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel, AdamW, BertConfig, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "movie_token = {}\n",
    "for movieId, text in movie_dict.items():\n",
    "  movie_token[movieId] = {}\n",
    "  encoded_dict = tokenizer.encode_plus(\n",
    "    text=movie_text[movieId],                      # Sentence to encode.\n",
    "    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "    max_length = 210,           # Pad & truncate all sentences.\n",
    "    pad_to_max_length = True,\n",
    "    return_attention_mask = True,   # Construct attn. masks\n",
    "    return_tensors = 'pt',     # Return pytorch tensors.\n",
    "  )\n",
    "  movie_token[movieId]['input_ids'] = encoded_dict['input_ids']\n",
    "  movie_token[movieId]['attention_mask'] = encoded_dict['attention_mask']\n",
    "\n",
    "# print(movie_token[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset, LatentFactorModel\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel, AdamW, BertConfig, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "  \"\"\"\n",
    "    x: [userId, movieId, rating, timestamp, datetime]\n",
    "  \"\"\"\n",
    "  def __init__(self, df_sample, token_dict, year_dict):\n",
    "    super(MyDataset, self).__init__()\n",
    "    self.model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    self.model.to(device)\n",
    "    self.df_sample = df_sample.values\n",
    "    # print(self.df_sample)\n",
    "    self.token_dict = token_dict\n",
    "    self.year_dict = year_dict\n",
    "  \n",
    "\n",
    "  def __len__(self):\n",
    "    # print(self.df_sample)\n",
    "    return len(self.df_sample)\n",
    "\n",
    "  # return cls, userId, year-diff, label\n",
    "  def __getitem__(self, idx):\n",
    "    # print(idx, len(self.df_sample))\n",
    "    sample = self.df_sample[idx]\n",
    "    userId, movieId, score, time = sample[0], sample[1], sample[2], sample[4]\n",
    "    token = self.token_dict[movieId]\n",
    "    input_ids, attention_mask = token['input_ids'].to(device), token['attention_mask'].to(device)\n",
    "    CLS = self.model(input_ids, token_type_ids=None, attention_mask=attention_mask)[1]\n",
    "    # print(CLS.shape)\n",
    "    year_diff = time.year - self.year_dict[movieId]\n",
    "    return CLS.to(device), userId, year_diff, score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LatentFactorModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LatentFactorModel(nn.Module):\n",
    "    def __init__(self, num_users, latent_dim):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        self.userEmbedding = nn.Embedding(num_users+1, latent_dim)\n",
    "        self.userEmbedding.weight.data.uniform_(-1, 1)\n",
    "        self.w_year = Variable(torch.tensor(0.5), requires_grad=True).to(device)\n",
    "        self.w_dot = Variable(torch.tensor(0.5), requires_grad=True).to(device)\n",
    "        self.bias = Variable(torch.tensor(3.0), requires_grad=True).to(device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, userId, movie_vec, year_diff):\n",
    "        user_vec = self.userEmbedding(userId)\n",
    "        user_vec = torch.unsqueeze(user_vec, 1)\n",
    "        movie_vec = torch.permute(movie_vec, (0, 2, 1))\n",
    "        ret = (self.sigmoid(torch.bmm(user_vec, movie_vec).reshape(-1))+(1/9))*4.5 # + self.bias #+ self.w_year * year_diff \n",
    "        return ret"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import MSELoss\n",
    "def train(data, batch_size=16, shuffle=True, lr=0.001, num_epoch=100, movie_dict={}, movie_year={}, latent_dim=768):\n",
    "  train_data, val_data = data[0], data[1]\n",
    "  train_loader = DataLoader(MyDataset(train_data, movie_dict, movie_year), batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "  model = LatentFactorModel(num_users=num_users, latent_dim=latent_dim)\n",
    "  model.to(device)\n",
    "  model.train()\n",
    "  optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, weight_decay=0.01)\n",
    "#   optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr, weight_decay=0.01)\n",
    "#   optimizer = nn.DataParallel(optimizer, device_ids=device_ids)\n",
    "#   optimizer.to(device)\n",
    "\n",
    "\n",
    "  mse_loss = nn.MSELoss()\n",
    "  # crossEntropy = nn.CrossEntropyLoss()\n",
    "  for i in range(num_epoch):\n",
    "    print(\"================{i} iteration================ \".format(i=i + 1))\n",
    "    predList = np.array([])\n",
    "    labelList = np.array([])\n",
    "    for j, sample in enumerate(train_loader):\n",
    "      CLS, userId, year_diff, score = sample[0].to(device), sample[1].to(device), sample[2].to(device), sample[3].to(device)\n",
    "      optimizer.zero_grad()\n",
    "      pred = model(userId, CLS, year_diff)\n",
    "      # print(pred, score)\n",
    "      loss = mse_loss(pred, score.float())\n",
    "      predList = np.append(predList, pred.detach().cpu().numpy())\n",
    "      labelList = np.append(labelList, score.detach().cpu().numpy())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if (j+1)%500 == 0:\n",
    "        print(loss.item())\n",
    "    total_loss = mse_loss(torch.tensor(predList), torch.tensor(labelList))\n",
    "    print(\"training loss={loss}\".format(loss=loss))\n",
    "    model.eval()\n",
    "    eval(val_data, model, batch_size, shuffle, movie_dict, movie_year, latent_dim)\n",
    "    model.train()\n",
    "    if (i)%20 == 0:\n",
    "      state = {\n",
    "        'epoch': (i+1),\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "      }\n",
    "      filepath = \"lfm_epoch{epoch}\".format(epoch=(i+1))\n",
    "      torch.save(state, \"./model/{filepath}.pt\".format(filepath=filepath))\n",
    "      # state = torch.load(filepath)\n",
    "      # model.load_state_dict(state['state_dict'])\n",
    "      # optimizer.load_state_dict(state['optimizer'])\n",
    "      "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Eval"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval(val_data, model, batch_size=1, shuffle=True, movie_dict={}, movie_year={}, latent_dim=768):\n",
    "  val_loader = DataLoader(MyDataset(val_data, movie_dict, movie_year), batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "  mse_loss = nn.MSELoss()\n",
    "  # crossEntropy = nn.CrossEntropyLoss()\n",
    "  predList = np.array([])\n",
    "  labelList = np.array([])\n",
    "  for j, sample in enumerate(val_loader):\n",
    "    CLS, userId, year_diff, score = sample[0].to(device), sample[1].to(device), sample[2].to(device), sample[3].to(device)\n",
    "    pred = model(userId, CLS, year_diff)\n",
    "#    print(pred, score)\n",
    "    predList = np.append(predList, pred.detach().cpu().numpy())\n",
    "    labelList = np.append(labelList, score.detach().cpu().numpy())\n",
    "\n",
    "  val_loss = mse_loss(torch.tensor(predList), torch.tensor(labelList))\n",
    "  print(\"validate loss={loss}\".format(loss=val_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(data=(train_data, val_data), batch_size=1, shuffle=True, lr=0.001, num_epoch=1, movie_dict=movie_token, movie_year=movie_year, latent_dim=768)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movieSet = set(movies['movieId'])\n",
    "tagMovieSet = set(tags['movieId'])\n",
    "ratingMovieSet = set(ratings['movieId'])\n",
    "print(len(movieSet), len(tagMovieSet), len(ratingMovieSet))\n",
    "print(len(movieSet.intersection(tagMovieSet)))\n",
    "print(len(movieSet.intersection(ratingMovieSet)))\n",
    "print(len(ratingMovieSet.intersection(tagMovieSet)))\n",
    "a = torch.zeros(2,3).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi -l 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LFM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}